{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOH3r7pV3mbYbcNXf2HJVG2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"],"metadata":{"id":"IFsRTL4JNhUp"}},{"cell_type":"markdown","source":["Ans. Ridge regression, also known as L2 regularized regression, is a linear regression technique that introduces a penalty term to the ordinary least squares (OLS) regression objective function. The penalty term discourages overly complex models by shrinking the regression coefficients. This helps address the problem of multicollinearity (when predictor variables are highly correlated) and overfitting (when the model fits the training data too closely but performs poorly on new data).\n","\n","The Ridge regression objective function is:\n","\n","Minimize:\n","∑\n","�\n","=\n","1\n","�\n","(\n","�\n","�\n","−\n","�\n","^\n","�\n",")\n","2\n","+\n","�\n","∑\n","�\n","=\n","1\n","�\n","�\n","�\n","2\n","Minimize:\n","i=1\n","∑\n","n\n","​\n"," (y\n","i\n","​\n"," −\n","y\n","^\n","​\n","  \n","i\n","​\n"," )\n","2\n"," +λ\n","j=1\n","∑\n","p\n","​\n"," β\n","j\n","2\n","​\n"],"metadata":{"id":"ULAOD3vINk7n"}},{"cell_type":"markdown","source":["Q2. What are the assumptions of Ridge Regression?"],"metadata":{"id":"SlHKIYh-N2VE"}},{"cell_type":"markdown","source":["Ans. When predictor variables are highly correlated.\n","When you suspect overfitting in your model.\n","When you need a balance between bias and variance."],"metadata":{"id":"7ibBGs0mN-Y9"}},{"cell_type":"markdown","source":["Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"],"metadata":{"id":"MeWb7Am3ONyk"}},{"cell_type":"markdown","source":["Ans. Selecting the value of the tuning parameter (\n","�\n","λ) in Ridge Regression is critical because it controls the balance between the penalty applied to the coefficients and the goodness of fit of the model. An optimal\n","�\n","λ value ensures the model generalizes well to unseen data"],"metadata":{"id":"szG6w1yMOTfH"}},{"cell_type":"markdown","source":["Q4. Can Ridge Regression be used for feature selection? If yes, how?"],"metadata":{"id":"_h_HKBqvOlOh"}},{"cell_type":"markdown","source":["Ans.Ridge regression is not inherently designed for feature selection, as it shrinks the coefficients of less important features toward zero but typically does not set them exactly to zero. This is in contrast to Lasso regression, which can set coefficients to zero due to its\n","�\n","1\n","L\n","1\n","​\n","  regularization penalty."],"metadata":{"id":"2lGuwtNJOo_H"}},{"cell_type":"markdown","source":["Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"],"metadata":{"id":"E89CyvIzO06k"}},{"cell_type":"markdown","source":["Ans. In ordinary least squares (OLS) regression, multicollinearity occurs when predictor variables are highly correlated.\n","It causes the following issues:\n","Unstable coefficients: Small changes in data can lead to large changes in the estimated coefficients.\n","High variance: The model becomes sensitive to noise, leading to unreliable predictions.\n","Interpretability problems: Coefficients lose their individual significance because correlated predictors share information."],"metadata":{"id":"fTRUI_vAO4L0"}},{"cell_type":"markdown","source":[],"metadata":{"id":"okOSUASoPHn3"}}]}